{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parser for Aspect Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada tahapan kedua ini akan dilakukan data parsing. Data parsing yang dimaksud adalah melakukan pembagian data dari berbentuk review string biasa menjadi kolom-kolom word_before, word_now, word_after, dan pos_tag nya serta class nya. Parsing ini harus dilakukan sebelum melakukan training atau testing. Di dalam data parsing ini digunakan program posTagger milik anggota kelompok kami yakni Nixon Andhika."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ferdy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     C:\\Users\\ferdy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from posTagger import posTagger\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/data-aspect.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pos Tagger Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagger = posTagger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_string_to_list(string_list):\n",
    "    splitted_strings = []\n",
    "    for string_comp in string_list:\n",
    "        splitted_strings.append(string_comp.split())\n",
    "    return splitted_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x00000213A63589D8>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\ferdy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 532, in __del__\n",
      "  File \"c:\\users\\ferdy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 5223, in device\n",
      "    return context.device(device_name_or_function)\n",
      "  File \"c:\\users\\ferdy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 2037, in device\n",
      "    return context().device(name)\n",
      "  File \"c:\\users\\ferdy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 842, in device\n",
      "    elif pydev.is_device_spec(name):\n",
      "  File \"c:\\users\\ferdy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\device.py\", line 47, in is_device_spec\n",
      "    return isinstance(obj, device_spec.DeviceSpecV2)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-958ba433a83a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0marr_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mstring_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_string_to_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'review'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mpos_tagged_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos_tagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ITB Jurusan IF\\Tahun Keempat\\Pemrosesan Bahasa Alami\\Tubes\\Steam-Games-Review-Sentiment-Analysis\\posTagger.py\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(self, token_list)\u001b[0m\n\u001b[0;32m     62\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                         \u001b[0mtag_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "aspect_data = pd.DataFrame()\n",
    "arr_word_before = []\n",
    "arr_word_now = []\n",
    "arr_word_after = []\n",
    "arr_pos_tag = []\n",
    "arr_class = []\n",
    "string_list = split_string_to_list(data['review'])\n",
    "pos_tagged_data = pos_tagger.pos_tag(string_list)\n",
    "\n",
    "for i in range(500):\n",
    "    aspect = data['aspect_keywords'][i]\n",
    "    \n",
    "    for j in range(len(pos_tagged_data[i])):\n",
    "        if (j == 0):\n",
    "            arr_word_before.append('[START]')\n",
    "        else:\n",
    "            arr_word_before.append(pos_tagged_data[i][j-1][0])\n",
    "        \n",
    "        arr_word_now.append(pos_tagged_data[i][j][0])\n",
    "        \n",
    "        arr_pos_tag.append(pos_tagged_data[i][j][1])\n",
    "        \n",
    "        if (j == (len(pos_tagged_data[i]) - 1)):\n",
    "            arr_word_after.append('[END]')\n",
    "        else:\n",
    "            arr_word_after.append(pos_tagged_data[i][j+1][0])\n",
    "        \n",
    "        if (pos_tagged_data[i][j][0] in aspect):\n",
    "            arr_class.append('true')\n",
    "        else:\n",
    "            arr_class.append('false')\n",
    "\n",
    "aspect_data['word_before'] = arr_word_before\n",
    "aspect_data['word_now'] = arr_word_now\n",
    "aspect_data['word_after'] = arr_word_after\n",
    "aspect_data['pos_tag'] = arr_pos_tag\n",
    "aspect_data['class'] = arr_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_data.to_csv('./data/steam-aspect.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
