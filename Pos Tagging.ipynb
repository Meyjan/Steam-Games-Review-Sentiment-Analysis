{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "# Pos Tagging\n",
    "\n",
    "Nixon Andhika / 13517059  \n",
    "Ferdy Santoso / 13517116  \n",
    "Jan Meyer Saragih / 13517131"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from nltk.corpus import wordnet, brown, treebank, conll2000\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import (\n",
    "    InputLayer, \n",
    "    LSTM, \n",
    "    Embedding, \n",
    "    TimeDistributed, \n",
    "    Dense, \n",
    "    Bidirectional, \n",
    "    Activation,\n",
    "    Dropout,\n",
    "    SimpleRNN\n",
    ")\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import backend\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.1\n",
    "VAL_SIZE = 0.15\n",
    "EPOCH_COUNT = 3\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset yang digunakan adalah dataset dari nltk corpus library. Di dataset, setiap kata telah dilabeli dengan POS Tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank_corpus = treebank.tagged_sents(tagset='universal')\n",
    "brown_corpus = brown.tagged_sents(tagset='universal')\n",
    "conll_corpus = conll2000.tagged_sents(tagset='universal')\n",
    "tagged_sentences = treebank_corpus + brown_corpus + conll_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Word and Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset yang diimpor memiliki data berupa tuple (word, tag) sehingga perlu dipisah terlebih dahulu. Setiap sentence words (list of word) dimasukkan ke variabel X sedangkan setiap sentence tags (list of tags) dimasukkan ke variabel Y. Selain itu, dibentuk list semua kata unik dari dataset yang disimpan dalam variabel words dan list semua tag unik yang disimpan dalam variabel tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    words_temp = []\n",
    "    tags_temp = []\n",
    "    for pair in sentence:         \n",
    "        words_temp.append(pair[0])\n",
    "        tags_temp.append(pair[1])\n",
    "    X.append(words_temp)\n",
    "    Y.append(tags_temp)\n",
    "\n",
    "words = set([word.lower() for sentence in X for word in sentence])\n",
    "tags = set([tag for sentence in Y for tag in sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dilakukan tokenisasi terhadap variabel X yang berisi sentence words dan variabel Y yang berisi sentence tags. Tokenisasi dilakukan dengan Tokenizer dari Keras. Dilakukan fit_on_texts untuk membentuk vocabulary index dari setiap kata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing words\n",
    "word_tokenizer = Tokenizer(lower=True, oov_token='<<OOV>>')\n",
    "word_tokenizer.fit_on_texts(X)\n",
    "\n",
    "# Tokenizing tags\n",
    "tag_tokenizer = Tokenizer(lower=False)\n",
    "tag_tokenizer.fit_on_texts(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasil tokenisasi yang masih berupa kata kemudian diubah menjadi sekuens integer menggunakan texts_to_sequences. Hasil yang didapatkan adalah hasil perubahan setiap kata menjadi indeksnya pada kamus dari Tokenizer. Untuk Tokenizer yang digunakan ke tag, ditambahkan satu entry '<<PAD>>' = 0 karena akan dilakukan padding dengan nilai 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words sequencing\n",
    "X_sequence = word_tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Tags sequencing\n",
    "Y_sequence = tag_tokenizer.texts_to_sequences(Y)\n",
    "\n",
    "# Adding PAD tag to dictionary\n",
    "tag_tokenizer.word_index['<<PAD>>'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Training Data and Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dilakukan split data menjadi training data dan testing data. Didefinisikan MAX_LENGTH untuk ukuran data yang akan dimasukkan ke network. Splitting dilakukan dengan train_test_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_, X_test_, Y_train_, Y_test_ = train_test_split(X_sequence, Y_sequence, test_size=TEST_SIZE)\n",
    "\n",
    "# Defining input layer size\n",
    "MAX_LENGTH = len(max(X_train_, key=len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karena Keras membutuhkan ukuran yang didefinisikan lebih dulu, dilakukan padding hingga MAX_LENGTH untuk menyamakan ukuran setiap data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = pad_sequences(X_train_, maxlen=MAX_LENGTH, padding='pre')\n",
    "X_test_ = pad_sequences(X_test_, maxlen=MAX_LENGTH, padding='pre')\n",
    "Y_train_ = pad_sequences(Y_train_, maxlen=MAX_LENGTH, padding='pre')\n",
    "Y_test_ = pad_sequences(Y_test_, maxlen=MAX_LENGTH, padding='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot Encoding dilakukan untuk merepresentasikan index tag menjadi list of bit sehingga dapat lebih dimengerti oleh model machine learning. One-Hot Encoding dilakukan menggunakan to_categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_ = to_categorical(Y_train_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arsitektur jaringan adalah sekuensial. Untuk eksperimen, dicoba model menggunakan RNN, LSTM, dan Bidirectional LSTM. Fungsi aktivasi yang digunakan adalah softmax dan digunakan layer Dropout untuk mengurangi overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 180, 128)          7609600   \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 180, 13)           1846      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 180, 13)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 180, 13)           182       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 180, 13)           0         \n",
      "=================================================================\n",
      "Total params: 7,611,628\n",
      "Trainable params: 7,611,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model = Sequential()\n",
    "rnn_model.add(InputLayer(input_shape=(MAX_LENGTH,)))\n",
    "rnn_model.add(Embedding(len(word_tokenizer.word_index)+1, 128))\n",
    "rnn_model.add(SimpleRNN(len(tag_tokenizer.word_index), return_sequences=True))\n",
    "rnn_model.add(Dropout(0.1))\n",
    "rnn_model.add(TimeDistributed(Dense(len(tag_tokenizer.word_index))))\n",
    "rnn_model.add(Activation('softmax'))\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "432/432 [==============================] - 119s 276ms/step - loss: 0.4589 - accuracy: 0.9329 - val_loss: 0.1240 - val_accuracy: 0.9782\n",
      "Epoch 2/3\n",
      "432/432 [==============================] - 118s 273ms/step - loss: 0.0857 - accuracy: 0.9858 - val_loss: 0.0529 - val_accuracy: 0.9910\n",
      "Epoch 3/3\n",
      "432/432 [==============================] - 122s 281ms/step - loss: 0.0441 - accuracy: 0.9928 - val_loss: 0.0339 - val_accuracy: 0.9932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24faabf6a30>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\n",
    "rnn_model.fit(X_train_, Y_train_, batch_size=BATCH_SIZE, epochs=EPOCH_COUNT, validation_split=VAL_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 180, 128)          7609600   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 180, 256)          394240    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 180, 256)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 180, 13)           3341      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 180, 13)           0         \n",
      "=================================================================\n",
      "Total params: 8,007,181\n",
      "Trainable params: 8,007,181\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model = Sequential()\n",
    "lstm_model.add(InputLayer(input_shape=(MAX_LENGTH,)))\n",
    "lstm_model.add(Embedding(len(word_tokenizer.word_index)+1, 128))\n",
    "lstm_model.add(LSTM(256, return_sequences=True))\n",
    "lstm_model.add(Dropout(0.1))\n",
    "lstm_model.add(TimeDistributed(Dense(len(tag_tokenizer.word_index))))\n",
    "lstm_model.add(Activation('softmax'))\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "432/432 [==============================] - 65s 149ms/step - loss: 0.1921 - accuracy: 0.9512 - val_loss: 0.0280 - val_accuracy: 0.9917\n",
      "Epoch 2/3\n",
      "432/432 [==============================] - 64s 147ms/step - loss: 0.0196 - accuracy: 0.9937 - val_loss: 0.0174 - val_accuracy: 0.9940\n",
      "Epoch 3/3\n",
      "432/432 [==============================] - 64s 147ms/step - loss: 0.0137 - accuracy: 0.9951 - val_loss: 0.0158 - val_accuracy: 0.9944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x250c97e3c10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\n",
    "lstm_model.fit(X_train_, Y_train_, batch_size=BATCH_SIZE, epochs=EPOCH_COUNT, validation_split=VAL_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 180, 128)          7609600   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 180, 512)          788480    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 180, 512)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 180, 13)           6669      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 180, 13)           0         \n",
      "=================================================================\n",
      "Total params: 8,404,749\n",
      "Trainable params: 8,404,749\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bi_lstm_model = Sequential()\n",
    "bi_lstm_model.add(InputLayer(input_shape=(MAX_LENGTH,)))\n",
    "bi_lstm_model.add(Embedding(len(word_tokenizer.word_index)+1, 128))\n",
    "bi_lstm_model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "bi_lstm_model.add(Dropout(0.1))\n",
    "bi_lstm_model.add(TimeDistributed(Dense(len(tag_tokenizer.word_index))))\n",
    "bi_lstm_model.add(Activation('softmax'))\n",
    "bi_lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "432/432 [==============================] - 88s 204ms/step - loss: 0.1793 - accuracy: 0.9480 - val_loss: 0.0231 - val_accuracy: 0.9933\n",
      "Epoch 2/3\n",
      "432/432 [==============================] - 87s 201ms/step - loss: 0.0154 - accuracy: 0.9951 - val_loss: 0.0137 - val_accuracy: 0.9955\n",
      "Epoch 3/3\n",
      "432/432 [==============================] - 88s 203ms/step - loss: 0.0096 - accuracy: 0.9968 - val_loss: 0.0123 - val_accuracy: 0.9959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x250cafb7340>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_lstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\n",
    "bi_lstm_model.fit(X_train_, Y_train_, batch_size=BATCH_SIZE, epochs=EPOCH_COUNT, validation_split=VAL_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berdasarkan hasil eksperimen, terlihat bahwa arsitektur Bidirectional LSTM menghasilkan nilai akurasi yang paling tinggi. Nilai akurasi masing-masing arsitektur adalah 99.28% untuk RNN, 99.51% untuk LSTM, 99.68% untuk Bidirectional LSTM. Hal ini mungkin terjadi karena kompleksitas arsitektur yang berbeda. Arsitektur RNN memiliki jaringan yang paling sederhana dibandingan LSTM dan Bidirectional LSTM. LSTM dan Bidirectional LSTM menggunakan multiple network layer sehingga lebih dapat memahami hubungan antar data. Bidirectional LSTM yang merupakan extension dari LSTM memiliki akurasi yang lebih baik dari LSTM karena lebih cocok untuk sequence classification problem seperti pada pelatihan model POS tagging.\n",
    "\n",
    "Pada RNN, juga terdapat vanishing gradient problem yang dapat menyebabkan jaringan berhenti training karena nilai gradient yang sangat kecil. LSTM dan Bidirectional LSTM menggunakan identity function untuk mengatasi masalah tersebut sehingga model terus dilatih yang menyebabkan nilai akurasi mungkin meningkat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model and Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model yang dihasilkan kemudian disimpan ke sebuah file h5 dan Tokenizer serta MAX_LENGTH disimpan ke file pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_lstm_model.save(\"model/bi_lstm_model.h5\")\n",
    "\n",
    "pickle_files = [word_tokenizer, tag_tokenizer, MAX_LENGTH]\n",
    "\n",
    "if not os.path.exists('PickledData/'):\n",
    "    os.makedirs('PickledData/')\n",
    "\n",
    "with open('PickledData/data.pkl', 'wb') as f:\n",
    "    pickle.dump(pickle_files, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing Pos Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = [\n",
    "    ['skyrim', 'nt', 'good', 'game', 'without', 'mods', 'fact', 'might', 'pay', 'mods', 'make', 'bugthesda', 's', 'game', 'playable', 'rubbish'],\n",
    "    ['addictive', 'game', 'ever', 'made'],\n",
    "    ['counter', 'strike', 'even', 'fight', 'highly', 'trained', 'american', 'antiterrorist', 'team', 'using', 'latest', 'military', 'technology', 'battle', 'group', 'really', 'madmen', 'possessing', 'crude', 'bomb', 'surplus', 'ussr', 's', 'army', 'supplies', 'despite', 'training', 'technology', 'terrorists', 'still', 'good', 'chance', 'blowing', 'market', 'therefore', 'much', 'like', 'real', 'life', 'game', 'currently', 'full', 'hackers', 'fly', 'top', 'map', 'unless', 'hack', 'like', 'getting', 'aerial', 'teabag', 'please', 'play', 'better', 'counter', 'strike', 'sauce', 'counter', 'strike', 'go', 'game', 'game', 'day', 'exists', 'historical', 'purposes', 'remember', 'times', 'internet', 'cafe', 'mosque', 'full', 'game']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        word2int, tag2int, MAX_LENGTH = pickle.load(f)\n",
    "        return word2int, tag2int, MAX_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk melakukan tagging, digunakan method pos_tag(). Algoritma diawali dengan me-load model dari file h5 dan me-load variabel Tokenizer serta MAX_LENGTH dari file pickle. Input yang berupa list of list of token kemudian diubah menjadi sekuens integer menggunakan Tokenizer yang di-load. Sekuens yang didapat di-padding hingga sebesar MAX_LENGTH. Model kemudian melakukan prediksi dengan menggunakan predict(). \n",
    "\n",
    "Kamus tag yang ada pada tag_tokenizer (Tokenizer POS TAG) di-reverse sehingga key menjadi index dan value menjadi kata POS TAG. Hasil tag didapatkan dengan memanggil sequences_to_tags() dengan parameter hasil prediksi dan kamus yang telah di-reverse. sequence_to_tags() akan mengembalikan value dari key dengan key berupa indeks prediksi dengan probabilitas terbesar.\n",
    "\n",
    "Setelah hasil tag didapatkan, dilakukan pengecekan panjang sentence asli dengan panjang sentence tag. Jika sentence tag yang didapatkan kurang panjang, dilakukan penambahan 'NOUN' di depan karena berdasarkan percobaan terdapat beberapa kasus 'NOUN' di awal sentence hilang. Setelah panjang keduanya sama, dibentuk tuple (word, tag) yang disimpan pada list result. Hasil list result kemudian dikembalikan sebagai hasil sentence yang telah di-tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(token_list):\n",
    "    bi_lstm_model = load_model(\"model/bi_lstm_model.h5\")\n",
    "    word_tokenizer, tag_tokenizer, MAX_LENGTH = load('PickledData/data.pkl')\n",
    "\n",
    "    input_sequences = word_tokenizer.texts_to_sequences(token_list)\n",
    "    input_sequences = pad_sequences(input_sequences, maxlen=MAX_LENGTH, padding='pre')\n",
    "    predictions = bi_lstm_model.predict(input_sequences)\n",
    "\n",
    "    reverse_tag_map = dict(map(reversed, tag_tokenizer.word_index.items()))\n",
    "    tag_result = sequences_to_tags(predictions, reverse_tag_map)\n",
    "\n",
    "    result = []\n",
    "    for i in range(len(token_list)):\n",
    "        if (len(token_list[i]) != len(tag_result[i])):\n",
    "            diff = len(token_list[i]) - len(tag_result[i])\n",
    "            if (diff > 0):\n",
    "                for j in range(diff):\n",
    "                    tag_result[i].insert(0, 'NOUN')\n",
    "        result.append(list(zip(token_list[i], tag_result[i])))\n",
    "    \n",
    "    return result\n",
    "\n",
    "def sequences_to_tags(predictions, tag_map):\n",
    "    tag_result = []\n",
    "    for prediction in predictions:\n",
    "        not_padding = False\n",
    "        tag_list = []\n",
    "        for index in prediction:\n",
    "            tag = tag_map[np.argmax(index)]\n",
    "            if (tag != \"<<PAD>>\"):\n",
    "                not_padding = True\n",
    "            if (not_padding):\n",
    "                tag_list.append(tag)\n",
    "\n",
    "        tag_result.append(tag_list)\n",
    "\n",
    "    return tag_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing Pos Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('skyrim', 'NOUN'), ('nt', 'NOUN'), ('good', 'ADJ'), ('game', 'NOUN'), ('without', 'ADP'), ('mods', 'NOUN'), ('fact', 'NOUN'), ('might', 'VERB'), ('pay', 'VERB'), ('mods', 'NOUN'), ('make', 'VERB'), ('bugthesda', 'NOUN'), ('s', 'NOUN'), ('game', 'NOUN'), ('playable', 'ADJ'), ('rubbish', 'NOUN')]\n",
      "[('addictive', 'NOUN'), ('game', 'NOUN'), ('ever', 'ADV'), ('made', 'VERB')]\n",
      "[('counter', 'NOUN'), ('strike', 'NOUN'), ('even', 'NOUN'), ('fight', 'NOUN'), ('highly', 'NOUN'), ('trained', 'ADV'), ('american', 'VERB'), ('antiterrorist', 'ADV'), ('team', 'VERB'), ('using', 'ADJ'), ('latest', 'NOUN'), ('military', 'NOUN'), ('technology', 'NOUN'), ('battle', 'ADJ'), ('group', 'NOUN'), ('really', 'NOUN'), ('madmen', 'NOUN'), ('possessing', 'NOUN'), ('crude', 'ADV'), ('bomb', 'ADJ'), ('surplus', 'NOUN'), ('ussr', 'ADJ'), ('s', 'NOUN'), ('army', 'NOUN'), ('supplies', 'NOUN'), ('despite', 'NOUN'), ('training', 'NOUN'), ('technology', 'NOUN'), ('terrorists', 'ADP'), ('still', 'NOUN'), ('good', 'NOUN'), ('chance', 'NOUN'), ('blowing', 'ADV'), ('market', 'ADJ'), ('therefore', 'NOUN'), ('much', 'NOUN'), ('like', 'NOUN'), ('real', 'ADV'), ('life', 'ADJ'), ('game', 'ADJ'), ('currently', 'ADJ'), ('full', 'NOUN'), ('hackers', 'NOUN'), ('fly', 'ADV'), ('top', 'ADJ'), ('map', 'NOUN'), ('unless', 'NOUN'), ('hack', 'ADJ'), ('like', 'NOUN'), ('getting', 'ADP'), ('aerial', 'NOUN'), ('teabag', 'ADJ'), ('please', 'NOUN'), ('play', 'ADJ'), ('better', 'NOUN'), ('counter', 'VERB'), ('strike', 'VERB'), ('sauce', 'ADJ'), ('counter', 'NOUN'), ('strike', 'NOUN'), ('go', 'VERB'), ('game', 'NOUN'), ('game', 'NOUN'), ('day', 'VERB'), ('exists', 'NOUN'), ('historical', 'NOUN'), ('purposes', 'NOUN'), ('remember', 'VERB'), ('times', 'ADJ'), ('internet', 'NOUN'), ('cafe', 'VERB'), ('mosque', 'NOUN'), ('full', 'NOUN'), ('game', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "result = pos_tag(test_samples)\n",
    "for res in result:\n",
    "    print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
